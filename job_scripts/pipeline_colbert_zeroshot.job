#!/bin/bash

#SBATCH --partition=gpu_h100
#SBATCH --gpus=1
#SBATCH --job-name=ColBERTZeroShot
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=9
#SBATCH --time=4:00:00
#SBATCH --output=./job_scripts/out/ColBERTZeroShot_%A.out

module purge
module load 2025

cd $HOME/ir2-colbert-news
curl -LsSf https://astral.sh/uv/install.sh | sh
export PATH="$HOME/.local/bin:$PATH"
uv sync --locked

# pip uninstall -y torch torchvision torchaudio
# pip install --index-url https://download.pytorch.org/whl/cu124 \
#   torch==2.6.0 torchvision torchaudio

uv run python - <<'EOF'
import torch
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU device count: {torch.cuda.device_count()}")
    print(f"Active GPU: {torch.cuda.get_device_name(0)}")
EOF
echo ""

# Load environment variables from .env if it exists
if [ -f .env ]; then
    export $(grep -v '^#' .env | xargs)
fi

# Check if data exists, if not run data setup and preprocessing
if [ ! -f "data/tiny/train/behaviors_parsed.tsv" ]; then
    echo "Data not found at data/tiny/train/behaviors_parsed.tsv"
    echo "Running data download and setup..."
    
    # 1. Download and Setup (Raw -> Original)
    # This script downloads MINDsmall and splits it into train/val/test in data/original
    srun uv run python baseline/download_and_setup_data.py --data_dir data/original
    
    echo "Running data preprocessing..."
    # This script processes text, tokenizes, and saves to data/tiny/train etc.
    srun uv run python baseline/data_preprocess.py --original_data_path data/original
fi

export TOKENIZERS_PARALLELISM=false
export MODEL_NAME=ColBERT
export BERT_MODEL=prajjwal1/bert-tiny

echo "Starting Zero-Shot (Frozen Weights) Training..."
echo "Training only the scoring layer with LR=1e-3"

srun uv run python baseline/train.py \
    --current_data_path data \
    --model_type $MODEL_NAME \
    --pretrained_model_name $BERT_MODEL \
    --colbert_embedding_dim 128 \
    --colbert_max_query_tokens 32 \
    --colbert_max_doc_tokens 128 \
    --colbert_freeze_weights \
    --learning_rate 1e-3 \
    --batch_size 256 \
    --num_epochs 10 \
    --dropout_probability 0.2

# Evaluate after training
srun uv run python baseline/evaluate.py \
    --current_data_path data \
    --model_type $MODEL_NAME \
    --pretrained_model_name $BERT_MODEL \
    --colbert_embedding_dim 128 \
    --colbert_max_query_tokens 32 \
    --colbert_max_doc_tokens 128 \
    --colbert_freeze_weights \
    --batch_size 256

