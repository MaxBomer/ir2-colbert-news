{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886fbfd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 20000 unique users, 17 unique news\n",
      "val: 50000 unique users, 42415 unique news\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# >>> EDIT THIS to your dataset root:\n",
    "DATASET_PATH = Path(\"/home/scur1748/ir2-colbert-news/baseline/data\")\n",
    "\n",
    "def load_behaviors(split: str) -> pd.DataFrame:\n",
    "    \"\"\"Load behaviors.tsv or behaviors_parsed.tsv and standardize columns.\"\"\"\n",
    "    cand = [DATASET_PATH / split / \"behaviors.tsv\",\n",
    "            DATASET_PATH / split / \"behaviors_parsed.tsv\"]\n",
    "    path = next((p for p in cand if p.exists()), None)\n",
    "    if path is None:\n",
    "        raise FileNotFoundError(f\"{split}: behaviors file not found (looked for behaviors.tsv / behaviors_parsed.tsv)\")\n",
    "    # Try no-header then header\n",
    "    try:\n",
    "        df = pd.read_csv(path, sep=\"\\t\", header=None, dtype=str, engine=\"python\", quoting=3, on_bad_lines=\"skip\")\n",
    "        if df.shape[1] < 5:\n",
    "            raise ValueError(\"behaviors has <5 columns\")\n",
    "        df = df.iloc[:, :5]\n",
    "        df.columns = [\"ImpressionID\", \"UserID\", \"Time\", \"History\", \"Impressions\"]\n",
    "    except Exception:\n",
    "        df = pd.read_csv(path, sep=\"\\t\", header=0, dtype=str, engine=\"python\", quoting=3, on_bad_lines=\"skip\").fillna(\"\")\n",
    "        # Map likely names to standard\n",
    "        colmap = {}\n",
    "        names = [c.lower() for c in df.columns]\n",
    "        want = [\"impressionid\",\"userid\",\"time\",\"history\",\"impressions\"]\n",
    "        # Simple align left-to-right if needed\n",
    "        if not set(want).issubset(names):\n",
    "            df = df.iloc[:, :5]\n",
    "            df.columns = [\"ImpressionID\", \"UserID\", \"Time\", \"History\", \"Impressions\"]\n",
    "        else:\n",
    "            # normalize casing\n",
    "            ren = {}\n",
    "            for c in df.columns:\n",
    "                lc = c.lower()\n",
    "                if lc == \"impressionid\": ren[c] = \"ImpressionID\"\n",
    "                elif lc == \"userid\": ren[c] = \"UserID\"\n",
    "                elif lc == \"time\": ren[c] = \"Time\"\n",
    "                elif lc == \"history\": ren[c] = \"History\"\n",
    "                elif lc == \"impressions\": ren[c] = \"Impressions\"\n",
    "            df = df.rename(columns=ren)\n",
    "    return df.fillna(\"\")\n",
    "\n",
    "def load_news(split: str) -> pd.DataFrame:\n",
    "    \"\"\"Load news.tsv and return df with a 'news_id' column.\"\"\"\n",
    "    path = DATASET_PATH / split / \"news.tsv\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"{split}: news.tsv not found\")\n",
    "    # Try header then no-header\n",
    "    try:\n",
    "        df = pd.read_csv(path, sep=\"\\t\", header=0, dtype=str, engine=\"python\", quoting=3)\n",
    "    except Exception:\n",
    "        df = pd.read_csv(path, sep=\"\\t\", header=None, dtype=str, engine=\"python\", quoting=3)\n",
    "    df = df.fillna(\"\")\n",
    "    # Find news id column robustly\n",
    "    candidates = [c for c in df.columns if str(c).lower() in {\"news_id\",\"newsid\",\"nid\",\"id\",\"news\"}]\n",
    "    if candidates:\n",
    "        nid_col = candidates[0]\n",
    "    else:\n",
    "        # assume first column is news id\n",
    "        nid_col = df.columns[0]\n",
    "    # normalize to 'news_id'\n",
    "    if nid_col != \"news_id\":\n",
    "        df = df.rename(columns={nid_col: \"news_id\"})\n",
    "    return df\n",
    "\n",
    "def count_users_and_news(split: str):\n",
    "    beh = load_behaviors(split)\n",
    "    news = load_news(split)\n",
    "    users = beh[\"UserID\"].nunique()\n",
    "    news_count = news[\"news_id\"].nunique()\n",
    "    return users, news_count\n",
    "\n",
    "for split in [\"train\", \"val\"]:\n",
    "    try:\n",
    "        u, n = count_users_and_news(split)\n",
    "        print(f\"{split}: {u} unique users, {n} unique news\")\n",
    "    except Exception as e:\n",
    "        print(f\"{split}: ERROR -> {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28f4c06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Loading behaviors from /home/scur1748/ir2-colbert-news/baseline/data/train/behaviors.tsv\n",
      "[train] behavior rows: 156,965 | unique users: 50,000\n",
      "[train] users kept: 20,000\n",
      "[train] behaviors written -> /home/scur1748/ir2-colbert-news/baseline/data_downsampled_20k/train/behaviors.tsv (rows: 62,668)\n",
      "[train] referenced news ids (from behaviors): 38,411\n",
      "[train] Filtering news.tsv (stream, preserve JSON) ...\n",
      "[news] original lines read: 51,282 | kept lines: 38,411 | header=no\n",
      "[train] news.tsv written -> /home/scur1748/ir2-colbert-news/baseline/data_downsampled_20k/train/news.tsv | lines=38,411 | header=no\n",
      "[train] âœ… Done!\n",
      "\n",
      "[val] Copying validation split unchanged...\n",
      "[val] âœ… Done (unchanged).\n",
      "ðŸŽ‰ All done!\n"
     ]
    }
   ],
   "source": [
    "# --- ONE CELL: robust downsample that preserves news.tsv JSON exactly ---\n",
    "\n",
    "import os, re, shutil, random, json\n",
    "from typing import Set\n",
    "import pandas as pd\n",
    "\n",
    "# ------------ edit your paths/params here ------------\n",
    "in_root  = \"/home/scur1748/ir2-colbert-news/baseline/data\"\n",
    "out_root = \"/home/scur1748/ir2-colbert-news/baseline/data_downsampled_20k\"\n",
    "seed = 42\n",
    "train_users = 20000\n",
    "val_users   = -1     # set >0 to also downsample val\n",
    "# -----------------------------------------------------\n",
    "\n",
    "BEH_COLS = [\"impression_id\", \"user_id\", \"time\", \"click_history\", \"impressions\"]\n",
    "SEP = re.compile(r\"[,\\s|]+\")\n",
    "\n",
    "def read_behaviors(path: str) -> pd.DataFrame:\n",
    "    # behaviors.tsv in MIND is TSV w/out header (5 cols)\n",
    "    # Use python engine + QUOTE_NONE to avoid accidental quote parsing\n",
    "    return pd.read_csv(path, sep=\"\\t\", header=None, names=BEH_COLS,\n",
    "                       dtype=str, engine=\"python\", quoting=3, on_bad_lines=\"skip\")\n",
    "\n",
    "def extract_news_from_behaviors(df: pd.DataFrame) -> Set[str]:\n",
    "    \"\"\"Collect news IDs from click_history and impressions, trimming labels -0/-1 and odd separators.\"\"\"\n",
    "    news_ids: Set[str] = set()\n",
    "\n",
    "    # History: tokens like Nxxxx separated by space/comma/pipe; '-' means empty\n",
    "    for h in df[\"click_history\"].fillna(\"\").astype(str):\n",
    "        if not h or h == \"-\":\n",
    "            continue\n",
    "        for tok in SEP.split(h.strip()):\n",
    "            tok = tok.strip()\n",
    "            if not tok:\n",
    "                continue\n",
    "            if \"-\" in tok:  # safety: drop any stuck-on label\n",
    "                tok = tok.split(\"-\", 1)[0]\n",
    "            news_ids.add(tok)\n",
    "\n",
    "    # Impressions: tokens \"Nxxxx-0/1\" or \"Nxxxx\"\n",
    "    for imp in df[\"impressions\"].fillna(\"\").astype(str):\n",
    "        if not imp:\n",
    "            continue\n",
    "        for tok in SEP.split(imp.strip()):\n",
    "            tok = tok.strip()\n",
    "            if not tok:\n",
    "                continue\n",
    "            if \"-\" in tok:\n",
    "                tok = tok.split(\"-\", 1)[0]\n",
    "            news_ids.add(tok)\n",
    "\n",
    "    # Keep typical MIND IDs (start with 'N')\n",
    "    return {nid for nid in news_ids if nid and nid[0] == \"N\"}\n",
    "\n",
    "def sample_users(df: pd.DataFrame, target_users: int, seed: int) -> Set[str]:\n",
    "    users = sorted(df[\"user_id\"].astype(str).unique().tolist())\n",
    "    if target_users <= 0 or target_users >= len(users):\n",
    "        return set(users)\n",
    "    rnd = random.Random(seed)\n",
    "    return set(rnd.sample(users, target_users))\n",
    "\n",
    "def subset_behaviors(df: pd.DataFrame, keep_users: Set[str]) -> pd.DataFrame:\n",
    "    return df[df[\"user_id\"].astype(str).isin(keep_users)].copy()\n",
    "\n",
    "def filter_news_stream_preserve_json(news_path: str, keep_news_ids: Set[str], out_path: str):\n",
    "    \"\"\"\n",
    "    Stream-filter news.tsv by FIRST FIELD (news_id) and preserve the original line bytes:\n",
    "    - Writes header line through unchanged if present (starts with 'news_id\\\\t')\n",
    "    - For data lines, keeps the line if the first tab-delimited field is in keep_news_ids\n",
    "    Result: title/abstract and *_entities JSON are preserved exactly (no re-quoting).\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    kept = 0\n",
    "    total = 0\n",
    "    header_written = False\n",
    "    with open(news_path, \"r\", encoding=\"utf-8\") as fin, open(out_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        first = fin.readline()\n",
    "        if not first:\n",
    "            return\n",
    "        total += 1\n",
    "        if first.lower().startswith(\"news_id\\t\"):\n",
    "            fout.write(first)  # write header as-is\n",
    "            header_written = True\n",
    "        else:\n",
    "            # Not a header; process the first line as data\n",
    "            parts = first.rstrip(\"\\n\").split(\"\\t\", 1)\n",
    "            nid = parts[0]\n",
    "            if nid in keep_news_ids:\n",
    "                fout.write(first)\n",
    "                kept += 1\n",
    "\n",
    "        for line in fin:\n",
    "            total += 1\n",
    "            parts = line.rstrip(\"\\n\").split(\"\\t\", 1)\n",
    "            if not parts:\n",
    "                continue\n",
    "            nid = parts[0]\n",
    "            if header_written and nid.lower() == \"news_id\":\n",
    "                # rare duplicated header inside file; pass through\n",
    "                fout.write(line)\n",
    "                continue\n",
    "            if nid in keep_news_ids:\n",
    "                fout.write(line)\n",
    "                kept += 1\n",
    "\n",
    "    print(f\"[news] original lines read: {total:,} | kept lines: {kept:,} | header={'yes' if header_written else 'no'}\")\n",
    "\n",
    "def sanity_check_news_entities(news_path: str, max_issues: int = 10):\n",
    "    \"\"\"\n",
    "    Scan the filtered news.tsv and try to validate JSON in title_entities / abstract_entities columns\n",
    "    WITHOUT rewriting the file. This only reads/validates.\n",
    "    \"\"\"\n",
    "    # Try to read header to find columns; if no header, skip this deep check\n",
    "    with open(news_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        head = f.readline()\n",
    "        if not head:\n",
    "            print(\"[sanity] news.tsv empty.\")\n",
    "            return\n",
    "        has_header = head.lower().startswith(\"news_id\\t\")\n",
    "    if not has_header:\n",
    "        print(\"[sanity] No header detected in news.tsv; skipping JSON validation (downstream parser likely expects header).\")\n",
    "        return\n",
    "\n",
    "    # With header present, use pandas to read but DO NOT write back\n",
    "    df = pd.read_csv(news_path, sep=\"\\t\", header=0, dtype=str, engine=\"python\", quoting=3, on_bad_lines=\"skip\").fillna(\"\")\n",
    "    problems = []\n",
    "    for col in [c for c in df.columns if c.lower() in (\"title_entities\", \"abstract_entities\")]:\n",
    "        for idx, s in df[col].items():\n",
    "            v = (\"\" if pd.isna(s) else str(s)).strip()\n",
    "            if v == \"\" or v == \"None\":\n",
    "                continue\n",
    "            try:\n",
    "                json.loads(v)\n",
    "            except Exception as e:\n",
    "                problems.append((col, idx, str(e), v[:120]))\n",
    "                if len(problems) >= max_issues:\n",
    "                    break\n",
    "        if len(problems) >= max_issues:\n",
    "            break\n",
    "\n",
    "    if problems:\n",
    "        print(\"[sanity] JSON problems detected in news entity columns (first few):\")\n",
    "        for col, idx, err, snippet in problems[:max_issues]:\n",
    "            print(f\"  Column {col} | Row {idx}: {err} --> {snippet}\")\n",
    "        print(\"  (These indicate the SOURCE file had invalid JSON in *_entities; our filtering preserved lines as-is.)\")\n",
    "    else:\n",
    "        print(\"[sanity] Entity JSON looks OK in sampled rows.\")\n",
    "\n",
    "def process_split(split_name: str, in_root: str, out_root: str, seed: int, target_users: int):\n",
    "    in_split = os.path.join(in_root, split_name)\n",
    "    out_split = os.path.join(out_root, split_name)\n",
    "    os.makedirs(out_split, exist_ok=True)\n",
    "\n",
    "    beh_in  = os.path.join(in_split, \"behaviors.tsv\")\n",
    "    news_in = os.path.join(in_split, \"news.tsv\")\n",
    "    ent_in  = os.path.join(in_split, \"entity_embedding.vec\")\n",
    "    rel_in  = os.path.join(in_split, \"relation_embedding.vec\")\n",
    "\n",
    "    beh_out  = os.path.join(out_split, \"behaviors.tsv\")\n",
    "    news_out = os.path.join(out_split, \"news.tsv\")\n",
    "    ent_out  = os.path.join(out_split, \"entity_embedding.vec\")\n",
    "    rel_out  = os.path.join(out_split, \"relation_embedding.vec\")\n",
    "\n",
    "    print(f\"[{split_name}] Loading behaviors from {beh_in}\")\n",
    "    beh_df = read_behaviors(beh_in)\n",
    "    print(f\"[{split_name}] behavior rows: {len(beh_df):,} | unique users: {beh_df['user_id'].nunique():,}\")\n",
    "\n",
    "    keep_users = sample_users(beh_df, target_users, seed)\n",
    "    print(f\"[{split_name}] users kept: {len(keep_users):,}\")\n",
    "    beh_sub = subset_behaviors(beh_df, keep_users)\n",
    "\n",
    "    beh_sub.to_csv(beh_out, sep=\"\\t\", header=False, index=False)\n",
    "    print(f\"[{split_name}] behaviors written -> {beh_out} (rows: {len(beh_sub):,})\")\n",
    "\n",
    "    keep_news_ids = extract_news_from_behaviors(beh_sub)\n",
    "    print(f\"[{split_name}] referenced news ids (from behaviors): {len(keep_news_ids):,}\")\n",
    "\n",
    "    print(f\"[{split_name}] Filtering news.tsv (stream, preserve JSON) ...\")\n",
    "    filter_news_stream_preserve_json(news_in, keep_news_ids, news_out)\n",
    "\n",
    "    # quick sanity: count lines & header in output\n",
    "    with open(news_out, \"r\", encoding=\"utf-8\") as f:\n",
    "        first = f.readline()\n",
    "        has_header = first.lower().startswith(\"news_id\\t\")\n",
    "        nlines = 1 + sum(1 for _ in f) if first else 0\n",
    "    print(f\"[{split_name}] news.tsv written -> {news_out} | lines={nlines:,} | header={'yes' if has_header else 'no'}\")\n",
    "\n",
    "    # Validate entity JSON if header present\n",
    "    if has_header:\n",
    "        sanity_check_news_entities(news_out)\n",
    "\n",
    "    # Copy embeddings unchanged\n",
    "    if os.path.exists(ent_in):\n",
    "        shutil.copyfile(ent_in, ent_out)\n",
    "    if os.path.exists(rel_in):\n",
    "        shutil.copyfile(rel_in, rel_out)\n",
    "\n",
    "    print(f\"[{split_name}] âœ… Done!\\n\")\n",
    "\n",
    "# -------------------- run --------------------\n",
    "process_split(\"train\", in_root, out_root, seed, train_users)\n",
    "\n",
    "if val_users == -1:\n",
    "    print(\"[val] Copying validation split unchanged...\")\n",
    "    in_val = os.path.join(in_root, \"val\")\n",
    "    out_val = os.path.join(out_root, \"val\")\n",
    "    os.makedirs(out_val, exist_ok=True)\n",
    "    for fname in [\"behaviors.tsv\", \"news.tsv\", \"entity_embedding.vec\", \"relation_embedding.vec\"]:\n",
    "        src = os.path.join(in_val, fname)\n",
    "        dst = os.path.join(out_val, fname)\n",
    "        if os.path.exists(src):\n",
    "            shutil.copyfile(src, dst)\n",
    "    print(\"[val] âœ… Done (unchanged).\")\n",
    "else:\n",
    "    process_split(\"val\", in_root, out_root, seed, val_users)\n",
    "\n",
    "print(\"ðŸŽ‰ All done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
