{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "886fbfd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 50000 unique users, 51281 unique news\n",
      "val: 50000 unique users, 42415 unique news\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# >>> EDIT THIS to your dataset root:\n",
    "DATASET_PATH = Path(\"/home/scur1748/ir2-colbert-news/baseline/data\")\n",
    "\n",
    "def load_behaviors(split: str) -> pd.DataFrame:\n",
    "    \"\"\"Load behaviors.tsv or behaviors_parsed.tsv and standardize columns.\"\"\"\n",
    "    cand = [DATASET_PATH / split / \"behaviors.tsv\",\n",
    "            DATASET_PATH / split / \"behaviors_parsed.tsv\"]\n",
    "    path = next((p for p in cand if p.exists()), None)\n",
    "    if path is None:\n",
    "        raise FileNotFoundError(f\"{split}: behaviors file not found (looked for behaviors.tsv / behaviors_parsed.tsv)\")\n",
    "    # Try no-header then header\n",
    "    try:\n",
    "        df = pd.read_csv(path, sep=\"\\t\", header=None, dtype=str, engine=\"python\", quoting=3, on_bad_lines=\"skip\")\n",
    "        if df.shape[1] < 5:\n",
    "            raise ValueError(\"behaviors has <5 columns\")\n",
    "        df = df.iloc[:, :5]\n",
    "        df.columns = [\"ImpressionID\", \"UserID\", \"Time\", \"History\", \"Impressions\"]\n",
    "    except Exception:\n",
    "        df = pd.read_csv(path, sep=\"\\t\", header=0, dtype=str, engine=\"python\", quoting=3, on_bad_lines=\"skip\").fillna(\"\")\n",
    "        # Map likely names to standard\n",
    "        colmap = {}\n",
    "        names = [c.lower() for c in df.columns]\n",
    "        want = [\"impressionid\",\"userid\",\"time\",\"history\",\"impressions\"]\n",
    "        # Simple align left-to-right if needed\n",
    "        if not set(want).issubset(names):\n",
    "            df = df.iloc[:, :5]\n",
    "            df.columns = [\"ImpressionID\", \"UserID\", \"Time\", \"History\", \"Impressions\"]\n",
    "        else:\n",
    "            # normalize casing\n",
    "            ren = {}\n",
    "            for c in df.columns:\n",
    "                lc = c.lower()\n",
    "                if lc == \"impressionid\": ren[c] = \"ImpressionID\"\n",
    "                elif lc == \"userid\": ren[c] = \"UserID\"\n",
    "                elif lc == \"time\": ren[c] = \"Time\"\n",
    "                elif lc == \"history\": ren[c] = \"History\"\n",
    "                elif lc == \"impressions\": ren[c] = \"Impressions\"\n",
    "            df = df.rename(columns=ren)\n",
    "    return df.fillna(\"\")\n",
    "\n",
    "def load_news(split: str) -> pd.DataFrame:\n",
    "    \"\"\"Load news.tsv and return df with a 'news_id' column.\"\"\"\n",
    "    path = DATASET_PATH / split / \"news.tsv\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"{split}: news.tsv not found\")\n",
    "    # Try header then no-header\n",
    "    try:\n",
    "        df = pd.read_csv(path, sep=\"\\t\", header=0, dtype=str, engine=\"python\", quoting=3)\n",
    "    except Exception:\n",
    "        df = pd.read_csv(path, sep=\"\\t\", header=None, dtype=str, engine=\"python\", quoting=3)\n",
    "    df = df.fillna(\"\")\n",
    "    # Find news id column robustly\n",
    "    candidates = [c for c in df.columns if str(c).lower() in {\"news_id\",\"newsid\",\"nid\",\"id\",\"news\"}]\n",
    "    if candidates:\n",
    "        nid_col = candidates[0]\n",
    "    else:\n",
    "        # assume first column is news id\n",
    "        nid_col = df.columns[0]\n",
    "    # normalize to 'news_id'\n",
    "    if nid_col != \"news_id\":\n",
    "        df = df.rename(columns={nid_col: \"news_id\"})\n",
    "    return df\n",
    "\n",
    "def count_users_and_news(split: str):\n",
    "    beh = load_behaviors(split)\n",
    "    news = load_news(split)\n",
    "    users = beh[\"UserID\"].nunique()\n",
    "    news_count = news[\"news_id\"].nunique()\n",
    "    return users, news_count\n",
    "\n",
    "for split in [\"train\", \"val\"]:\n",
    "    try:\n",
    "        u, n = count_users_and_news(split)\n",
    "        print(f\"{split}: {u} unique users, {n} unique news\")\n",
    "    except Exception as e:\n",
    "        print(f\"{split}: ERROR -> {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28f4c06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] kept users: 20,000 | kept news: 38,341\n",
      "[val] kept users: 20,000 | kept news: 30,105\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# === Downsample MIND by users & ensure all expected files exist ===\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "# --- CONFIG: update these paths ---\n",
    "INPUT_ROOT  = Path(\"/home/scur1748/ir2-colbert-news/baseline/data\")       # your current MIND root\n",
    "OUTPUT_ROOT = Path(\"/home/scur1748/ir2-colbert-news/baseline/data_downsampled_20k\")  # where to write the 20k subset\n",
    "KEEP_USERS  = 20_000\n",
    "SEED        = 42\n",
    "# ----------------------------------\n",
    "\n",
    "IMPR_SPLIT = re.compile(r\"\\s+\")\n",
    "PAIR_SPLIT = re.compile(r\"[-:]\")\n",
    "\n",
    "def _read_behaviors_any(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Standardize behaviors to 5 columns: ImpressionID, UserID, Time, History, Impressions.\n",
    "    Works for both behaviors.tsv and behaviors_parsed.tsv variants when they contain those fields.\n",
    "    Falls back to best-effort renaming.\n",
    "    \"\"\"\n",
    "    # try no header\n",
    "    df = pd.read_csv(path, sep=\"\\t\", header=None, dtype=str, engine=\"python\", quoting=3, on_bad_lines=\"skip\")\n",
    "    if df.shape[1] >= 5:\n",
    "        df = df.iloc[:, :5]\n",
    "        df.columns = [\"ImpressionID\",\"UserID\",\"Time\",\"History\",\"Impressions\"]\n",
    "    else:\n",
    "        # try header-based\n",
    "        df = pd.read_csv(path, sep=\"\\t\", header=0, dtype=str, engine=\"python\", quoting=3, on_bad_lines=\"skip\")\n",
    "        expected = [\"ImpressionID\",\"UserID\",\"Time\",\"History\",\"Impressions\"]\n",
    "        for i, col in enumerate(df.columns[:5]):\n",
    "            if i < 5 and col not in expected:\n",
    "                df = df.rename(columns={col: expected[i]})\n",
    "        for col in expected:\n",
    "            if col not in df.columns:\n",
    "                df[col] = \"\"\n",
    "        df = df[expected]\n",
    "    return df.fillna(\"\")\n",
    "\n",
    "def _extract_news_from_row(history: str, impressions: str):\n",
    "    ids = set()\n",
    "    if history:\n",
    "        ids |= {n for n in IMPR_SPLIT.split(history.strip()) if n}\n",
    "    if impressions:\n",
    "        for tok in IMPR_SPLIT.split(impressions.strip()):\n",
    "            tok = tok.strip()\n",
    "            if not tok:\n",
    "                continue\n",
    "            parts = PAIR_SPLIT.split(tok)\n",
    "            if parts:\n",
    "                ids.add(parts[0])\n",
    "    return ids\n",
    "\n",
    "def _collect_news_ids(behaviors_df: pd.DataFrame):\n",
    "    keep = set()\n",
    "    for _, r in behaviors_df.iterrows():\n",
    "        keep |= _extract_news_from_row(r.get(\"History\",\"\"), r.get(\"Impressions\",\"\"))\n",
    "    return keep\n",
    "\n",
    "def _read_news_any(news_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read news.tsv even if there is no header with 'NewsID'.\n",
    "    Assumes first column is the news id if header missing.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(news_path, sep=\"\\t\", dtype=str, engine=\"python\", quoting=3)\n",
    "        if \"NewsID\" not in df.columns:\n",
    "            # rename first column to NewsID\n",
    "            first = df.columns[0]\n",
    "            df = df.rename(columns={first: \"NewsID\"})\n",
    "    except Exception:\n",
    "        df = pd.read_csv(news_path, sep=\"\\t\", header=None, dtype=str, engine=\"python\", quoting=3)\n",
    "        # pad to at least 8 columns for MIND-like format\n",
    "        while df.shape[1] < 8:\n",
    "            df[df.shape[1]] = \"\"\n",
    "        df = df.iloc[:, :8]\n",
    "        df.columns = [\"NewsID\",\"Category\",\"SubCategory\",\"Title\",\"Abstract\",\"URL\",\"TitleEntities\",\"AbstractEntities\"]\n",
    "    return df.fillna(\"\")\n",
    "\n",
    "def _filter_news_tables(split_dir: Path, out_dir: Path, keep_news: set):\n",
    "    news_path = split_dir / \"news.tsv\"\n",
    "    if news_path.exists():\n",
    "        news_df = _read_news_any(news_path)\n",
    "        f_news = news_df[news_df[\"NewsID\"].astype(str).isin(keep_news)].copy()\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "        f_news.to_csv(out_dir / \"news.tsv\", sep=\"\\t\", index=False, header=True)\n",
    "    # optional parsed\n",
    "    np_path = split_dir / \"news_parsed.tsv\"\n",
    "    if np_path.exists():\n",
    "        np_df = pd.read_csv(np_path, sep=\"\\t\", dtype=str, engine=\"python\", quoting=3).fillna(\"\")\n",
    "        # find the news id column\n",
    "        cand = [c for c in np_df.columns if c.lower() in (\"newsid\",\"news_id\",\"nid\",\"id\",\"news\")]\n",
    "        if cand:\n",
    "            col = cand[0]\n",
    "        else:\n",
    "            col = np_df.columns[0]\n",
    "        np_f = np_df[np_df[col].astype(str).isin(keep_news)].copy()\n",
    "        np_f.to_csv(out_dir / \"news_parsed.tsv\", sep=\"\\t\", index=False, header=True)\n",
    "\n",
    "def _subset_user2int(split_dir: Path, out_dir: Path, keep_users: set, remap=False):\n",
    "    u2i = split_dir / \"user2int.tsv\"\n",
    "    if not u2i.exists():\n",
    "        return {}, set()\n",
    "    try:\n",
    "        df = pd.read_csv(u2i, sep=\"\\t\", dtype=str, engine=\"python\", quoting=3)\n",
    "        cols = list(df.columns)\n",
    "        if len(cols) < 2:\n",
    "            raise ValueError\n",
    "        user_col = cols[0]\n",
    "        int_col  = cols[1]\n",
    "    except Exception:\n",
    "        df = pd.read_csv(u2i, sep=\"\\t\", header=None, dtype=str, engine=\"python\", quoting=3)\n",
    "        if df.shape[1] < 2:\n",
    "            raise ValueError(\"user2int.tsv must have at least two columns\")\n",
    "        df = df.iloc[:, :2]\n",
    "        df.columns = [\"user\",\"int\"]\n",
    "        user_col, int_col = \"user\", \"int\"\n",
    "\n",
    "    df = df.fillna(\"\")\n",
    "    f = df[df[user_col].astype(str).isin(keep_users)].copy()\n",
    "    if remap:\n",
    "        f = f.sort_values(by=user_col).reset_index(drop=True)\n",
    "        f[int_col] = np.arange(len(f)).astype(str)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    f.to_csv(out_dir / \"user2int.tsv\", sep=\"\\t\", index=False, header=True)\n",
    "\n",
    "    user_to_int = dict(zip(f[user_col].astype(str), f[int_col].astype(str)))\n",
    "    int_set = set(f[int_col].astype(str))\n",
    "    return user_to_int, int_set\n",
    "\n",
    "def _filter_behaviors_parsed_if_present(split_dir: Path, out_dir: Path, keep_users: set, keep_user_ints: set, user_to_int: dict):\n",
    "    bp = split_dir / \"behaviors_parsed.tsv\"\n",
    "    if not bp.exists():\n",
    "        return\n",
    "    df = pd.read_csv(bp, sep=\"\\t\", dtype=str, engine=\"python\", quoting=3).fillna(\"\")\n",
    "    cols_lower = {c.lower(): c for c in df.columns}\n",
    "\n",
    "    # 1) If it has a 'userid' (any case), filter on it\n",
    "    for key in (\"userid\",\"user_id\",\"user\"):\n",
    "        if key in cols_lower:\n",
    "            c = cols_lower[key]\n",
    "            df_f = df[df[c].astype(str).isin(keep_users)].copy()\n",
    "            df_f.to_csv(out_dir / \"behaviors_parsed.tsv\", sep=\"\\t\", index=False, header=True)\n",
    "            return\n",
    "\n",
    "    # 2) Else try to filter using user-int column if present\n",
    "    #    Find the column with the highest overlap with kept user ints\n",
    "    best_col, best_overlap = None, -1\n",
    "    for c in df.columns:\n",
    "        col_vals = set(df[c].astype(str).unique())\n",
    "        overlap = len(col_vals & keep_user_ints)\n",
    "        if overlap > best_overlap:\n",
    "            best_overlap, best_col = overlap, c\n",
    "\n",
    "    if best_col is not None and best_overlap > 0:\n",
    "        df_f = df[df[best_col].astype(str).isin(keep_user_ints)].copy()\n",
    "        df_f.to_csv(out_dir / \"behaviors_parsed.tsv\", sep=\"\\t\", index=False, header=True)\n",
    "    else:\n",
    "        # If we cannot identify a user column, just copy the original so the file exists.\n",
    "        shutil.copy2(bp, out_dir / \"behaviors_parsed.tsv\")\n",
    "\n",
    "def _copy_if_exists(src_dir: Path, dst_dir: Path, name: str):\n",
    "    p = src_dir / name\n",
    "    if p.exists():\n",
    "        dst_dir.mkdir(parents=True, exist_ok=True)\n",
    "        shutil.copy2(p, dst_dir / name)\n",
    "\n",
    "def _downsample_split(split: str):\n",
    "    split_in  = INPUT_ROOT / split\n",
    "    split_out = OUTPUT_ROOT / split\n",
    "    split_out.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 1) choose behaviors file (prefer behaviors.tsv)\n",
    "    behaviors_path = None\n",
    "    for fname in [\"behaviors.tsv\", \"behaviors_parsed.tsv\"]:\n",
    "        p = split_in / fname\n",
    "        if p.exists():\n",
    "            behaviors_path = p\n",
    "            break\n",
    "    if behaviors_path is None:\n",
    "        raise FileNotFoundError(f\"No behaviors file found in {split_in}\")\n",
    "\n",
    "    # 2) read & sample users\n",
    "    behaviors = _read_behaviors_any(behaviors_path)\n",
    "    users = behaviors[\"UserID\"].astype(str).unique().tolist()\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    k = min(KEEP_USERS, len(users))\n",
    "    keep_users = set(rng.choice(users, size=k, replace=False).tolist())\n",
    "\n",
    "    # 3) filter behaviors and write the same filename back\n",
    "    behaviors_f = behaviors[behaviors[\"UserID\"].astype(str).isin(keep_users)].copy()\n",
    "    # Write both variants for convenience:\n",
    "    # - If original was behaviors.tsv, write behaviors.tsv\n",
    "    # - Additionally, also write a filtered behaviors_parsed.tsv so you don't \"miss\" it\n",
    "    behaviors_f.to_csv(split_out / \"behaviors.tsv\", sep=\"\\t\", index=False, header=True)\n",
    "\n",
    "    # 4) collect referenced news and filter news files\n",
    "    keep_news = _collect_news_ids(behaviors_f)\n",
    "    _filter_news_tables(split_in, split_out, keep_news)\n",
    "\n",
    "    # 5) user2int: subset (no remap to avoid breaking int references)\n",
    "    user_to_int, keep_user_ints = _subset_user2int(split_in, split_out, keep_users, remap=False)\n",
    "\n",
    "    # 6) behaviors_parsed.tsv: filter if present (by UserID or by int overlap). If not identifiable, copy.\n",
    "    _filter_behaviors_parsed_if_present(split_in, split_out, keep_users, keep_user_ints, user_to_int)\n",
    "\n",
    "    # 7) Ensure mapping files exist in output (copy-through)\n",
    "    for name in [\"category2int.tsv\", \"entity2int.tsv\", \"word2int.tsv\",\n",
    "                 \"entity_embedding.vec\", \"relation_embedding.vec\"]:\n",
    "        _copy_if_exists(split_in, split_out, name)\n",
    "\n",
    "    print(f\"[{split}] kept users: {len(keep_users):,} | kept news: {len(keep_news):,}\")\n",
    "\n",
    "# --- run for train & val ---\n",
    "OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "for split in [\"train\", \"val\"]:\n",
    "    if (INPUT_ROOT / split).exists():\n",
    "        _downsample_split(split)\n",
    "    else:\n",
    "        print(f\"Skipping missing split: {split}\")\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fd9196",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
